---
layout: post
title: "Support Vector Machine - SVM"
author: optboy
categories: [Machine Learning]
---

서포트 벡터머신에 대한 세미나를 준비했다. 

- support vector machine?  
    - 고차원 데이터 분류문제에 좋은 성능을 보인다.
    - 이차방정식 문제로 formulation을 한다.
    - training data에 대해서 성능이 좋아야 하는데, 너무 좋으면 overfitting 문제가 생긴다. testing error에 대해서도 성능이 좋아햐 한다. 이걸 generalization ability라고 한다.
    - 근데 training ability와 generalization ability간에는 상충관계가 존재한다.
    - 하지만 SVM은 training error를 줄이면 testing error도 줄어드는 모델을 만들 수 있다.
    - SVM은 통계에 근거해 탄탄히 발전함.

    - 분류문제를 다룬다. (two-class)
    - 3차원 공간에서 hyperplane을 찾는 것.
        - 식은 $$w^Tx + b = 0$$이다.
        - w:nominal vector of the hyperplane, b: bias
        - $$w, b$$를 찾아야 한다.
    - 어떻게 찾는가? 두 클래스를 나누는 hyperplane은 무수히 많다.
    - 가장 좋은걸 찾아야 하는데, 그 기준은 무엇인가.
    - SVM에서 추구하는 건 margin이라는 개념.
        - training에서 margin을 최대화하는 hyperplane을 찾자. 그러면 generalization error은 줄어든다.
        - 그러면 예측 성능도 좋아지는 것이다. 

    - 그러면 margin이 뭐냐? 
        - 두 클래스가 있고, hyperplane이 있을 때, 각 클래스에서 가장 가까운 관측치에 대한 거리를 margin이라고 한다. 
        - 이 margin은 w(기울기)로 표현이 가능하다.

        - 한 클래스를 +, 한 클래스를 -라고 했을 때, + plane은 정확히 y값이 +1인 값을 갖는 x들이 +plane에 있는 거고, -1을 갖는 x들은 -plane에 있는 것. 1보다 크거나 같은 녀석들이 + 클래스에 속하고, -1보다 작거나 같은 녀석들이 -클래스에 속하게 된다. 
        - 이 마진을 최대화하는 hyper plane을 찾는 것. 2차원이라면 직선이 된다.
        - +에 있는 점은 -에 있는 점을 w방향으로 평행이동한 것과 같다. 

    - 수학적으로 margin을 알아보자.
        - 결국 $$\lambda = \frac{2}{w^tw}$$임.
        - p norm에 대해서 알아야 함. -> L2 norm은 유클리디안 거리임. 
        - 이런저런 유도를 통해 margin = $$\frac{2}{\lVert w \rVert_2} = \frac{2}{\sqrt{w^tw}}$$가 된다. 
        - 이걸 maximize해야하는데, 그 역수를 minimize해도 된다. 
        - 근데 L2 norm은 제곱근이 있어서 제곱을 시켜서 목적함수를 바꿔줄 수도 있다. 

        - 이거 convex optimization problem으로 풀 수 있다. 
        - 목적식과 제약식이 있음. 제약이 margin을 1보다는 크게 하는 것임.
        - 목적식이 2차함수이기 때문에 quadratic programming(QP)임. 근데 QP는 convex optimization임.
        - convex는 global optimal solution을 쉽게 찾을 수 있다. 

    - 이걸 Lagrangian multiplier으로 하나의 목적식으로 만들어 줄 수 있다. 
    - 이 목적식을 최소로 하는 문제를 풀면 된다. 미분하면 된다. 
    - 그러면 w에 대한 식이 구해지고, y와 알파의 관계를 알 수 있음.
    - 이것도 primal 목적식에 넣는다. 
    - 이것저것해서 최종적으로 알파에 대한 dual문제를 만들 수 있음. 
    - 목적식을 최대화 하는 알파를 찾는다.

    - dual문제가 최적해가 되기 위한 조건 - KKT조건
        - primal도 얻고, dual도 얻었으면 이러한 조건을 얻어야 한다. 
            1. stationarity : 미분해서 0이 되는 지점이 존재해야 한다.
            2. primal feasibilty
            3. dual feasibility
            4. complementary slackness : $$a_i(y_i(w^tx_i+b)-1) = 0$$

    - complementary slackness
        - 알파가 0보다 클 때 뒤에 있는게 0이어야 함. => $$y_i(w^tx_i+b) = 1$$이어야 함.
            - 이 경우가 plus-plane이나 minus-plane위에 있는 것 -> 이게 support vector임. 
        - 알파가 0이면 뒤에 있는 애들이 0이 아니어도 된다. 이게 support vector 뒤에 있는 애들임. 
        - margin을 구할 때 사용하는 것들은 support vector에 해당하는 것들이다.  

    - x가 support vector인 경우만 알파가 0보다 크거나 같으므로, support vector만 이용해서 Hyper plane을 구할 수 있다.
        - sparse representation

    - 이걸로 w랑 b구한다. 

    - 새로운 데이터가 optimal separating hyperplane보다 밑에 있으면 -1, 위에 있으면 1로 예측하여 class를 부여한다.

        
- 선형 분리 불가능한 경우의 선형 SVM -> soft margin SVM
    - 선형분리 가능하면 위에 설명한 것 -> hard margin SVM

    - 완벽하게 나뉘지 못하는 경우가 생긴다.
    - 이럴때는 hard margin SVM과 유사한 방법을 쓰나, error을 허용한다. 
    - 정규화에서 regularigation처럼 목적함수 뒤에 에러(slack variable)를 붙여준다.
    - 여기서 상수 C를 붙여서 penalty를 최대한 억제한다. 
    - C값을 크게하면 에러를 허용하지 않지만, overfit될 가능성이 있다.
    - C값을 작게하면 에러를 허용하게 되고, underfit될 가능성이 생긴다.

    - 여기서도 lagrangian relaxation을 하고, 목적함수 미분해서 0나오는 값을 찾는다. 
    - 아까랑 똑같은데 크사이 하나 더 생김. 이에 대한 식을 하나 얻을 수 있다. 총 3개 얻음.
    - 전개해서 포뮬레이션 만들면 알파가 C보다 작거나 같아야하는 제약만 더해짐. 

    - 역시 KKT condition
    - ...

- 비선형 SVM, kernal method
    - 기본 아이디어는 관측치 x를 더 높은 차원으로 변환시켜 분류한다. 
    - x들을 어떤 함수에 통과시켜서 z를 얻는다. 
    - p차원에서 q차원으로 바꿈. q가 훨씬 큼. 3차원에서 10차원 정도로 올려버림. 이렇게 바꾸고 선형으로 푼다. 
    - original space에서 feature space로 바꾸는 것임. 이러면 더 쉽게 계산할 수 있음. 

    - SVM lagrangian dual 포뮬레이션 보면 x에 대한 inner project(내적)이 있는데, 이걸 $$\phi$$를 이용해서 데이터를 변환한 후 한다. 내적은 element들끼리 곱해서 더하는 것. 
    - $$\phi$$의 형태를 알아야 하는데, 그걸 몰라도 되는게, kernal function.

    - 예제

    - 이런 저런 kernal function들이 있다. 

    - 예제. 


- 비선형 SVM

#### 참고 

- [wekipedia](https://ko.wikipedia.org/wiki)
- [ratsgo's blog](https://ratsgo.github.io/machine%20learning/2017/05/23/SVM/)